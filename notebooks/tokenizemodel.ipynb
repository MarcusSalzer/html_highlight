{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from importlib import reload\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn import metrics\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "\n",
    "from src import plotly_plots as pp\n",
    "from src import torch_util as tu\n",
    "from src import util\n",
    "\n",
    "reload(util)\n",
    "reload(pp)\n",
    "split_idx, split_date = util.load_split_idx()\n",
    "df_splits = {\n",
    "    k: util.dataset_to_df(d)\n",
    "    for k, d in util.load_dataset_splits(\n",
    "        split_idx, Path(\"../data/dataset.ndjson\")\n",
    "    ).items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"training data:\")\n",
    "token_count = df_splits[\"train\"][\"length\"].sum()\n",
    "char_count = df_splits[\"train\"][\"tokens\"].list.explode().str.split(\"\").explode().len()\n",
    "print(\n",
    "    f\"we have {len(df_splits['train'])} examples\",\n",
    "    f\" -> {token_count} tokens (avg {token_count / len(df_splits['train']):.1f} tokens/example)\",\n",
    "    f\" -> {char_count} characters (avg {char_count / token_count:.1f} chars/token)\",\n",
    "    sep=\"\\n\",\n",
    ")\n",
    "\n",
    "# split to characters\n",
    "char_examples = (\n",
    "    df_splits[\"train\"]\n",
    "    .select(\"tokens\", \"tags\")\n",
    "    .map_rows(\n",
    "        lambda row: util.split_to_chars(*row, only_starts=False),\n",
    "    )\n",
    ")\n",
    "char_examples.columns = [\"chars\", \"char_tags\"]\n",
    "char_examples = char_examples.with_columns(\n",
    "    name=df_splits[\"train\"][\"name\"], lang=df_splits[\"train\"][\"lang\"]\n",
    ")\n",
    "print(f\"splitted data {char_examples.columns}\")\n",
    "\n",
    "# print(\n",
    "#     f\"\\nthere are {len(char_counts)} unique characters\",\n",
    "#     f\" and {len(char_tag_counts)} unique tags\",\n",
    "# )\n",
    "\n",
    "# train_df, val_df = util.data_split(char_examples, 0.3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab for tokens\n",
    "vocab = [\"<pad>\", \"<unk>\"] + list(char_counts.keys())[:10]\n",
    "token2idx = {t: i for i, t in enumerate(vocab)}\n",
    "\n",
    "# tags\n",
    "tag_vocab = [\"<pad>\"] + list(char_tag_counts.keys())\n",
    "tag2idx = {t: i for i, t in enumerate(tag_vocab)}\n",
    "\n",
    "print(\"vocab (tokens):\", vocab)\n",
    "print(\"vocab (tags)  :\", tag_vocab)\n",
    "\n",
    "# Convert tokens and labels to indices\n",
    "# these are lists of lists!\n",
    "train_token_idx = [[token2idx.get(t, 1) for t in seq] for seq in train_df[\"chars\"]]\n",
    "train_tag_idx = [[tag2idx[t] for t in seq] for seq in train_df[\"char_tags\"]]\n",
    "# print(\"\\nlists of lists:\")\n",
    "# print(train_token_idx)\n",
    "# print(train_tag_idx)\n",
    "print(f\"\\ntraining examples of length: {[len(e) for e in train_token_idx]}\")\n",
    "\n",
    "# validation data\n",
    "val_token_idx = [[token2idx.get(t, 1) for t in seq] for seq in val_df[\"chars\"]]\n",
    "val_tag_idx = [[tag2idx[t] for t in seq] for seq in val_df[\"char_tags\"]]\n",
    "print(f\"validation examples of length: {[len(e) for e in val_token_idx]}\")\n",
    "\n",
    "train_token_tensors = tu.seqs2padded_tensor(train_token_idx)\n",
    "train_tag_tensors = tu.seqs2padded_tensor(train_tag_idx)\n",
    "val_token_tensors = tu.seqs2padded_tensor(val_token_idx)\n",
    "val_tag_tensors = tu.seqs2padded_tensor(val_tag_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, tokens, labels):\n",
    "        self.tokens = tokens\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.tokens[idx], self.labels[idx]\n",
    "\n",
    "\n",
    "# Create dataset and dataloader\n",
    "train_dataset = SequenceDataset(train_token_tensors, train_tag_tensors)\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "Can we tokenize the text using a classifier model?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "    def __init__(self, vocab_size, tagset_size, embedding_dim, hidden_dim):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, sentences):\n",
    "        embeds = self.embedding(\n",
    "            sentences\n",
    "        )  # Shape: (batch_size, seq_len, embedding_dim)\n",
    "        lstm_out, _ = self.lstm(embeds)  # Shape: (batch_size, seq_len, hidden_dim)\n",
    "        tag_scores = self.hidden2tag(\n",
    "            lstm_out\n",
    "        )  # Shape: (batch_size, seq_len, tagset_size)\n",
    "        return tag_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "embedding_dim = 4\n",
    "hidden_dim = 32\n",
    "vocab_size = len(vocab)\n",
    "tagset_size = len(tag_vocab)\n",
    "\n",
    "model = LSTMTagger(vocab_size, tagset_size, embedding_dim, hidden_dim)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "# Training loop\n",
    "epochs = 20\n",
    "losses = []\n",
    "for epoch in range(epochs):\n",
    "    for examples, labels in train_loader:\n",
    "        # Forward pass\n",
    "        tag_scores = model(examples)\n",
    "\n",
    "        # Reshape for loss calculation\n",
    "        loss = loss_function(tag_scores.view(-1, tagset_size), labels.view(-1))\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    # print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}\")\n",
    "# pp.scatter(y=losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    tag_scores = model(val_token_tensors)\n",
    "    predictions = torch.argmax(tag_scores, dim=-1)  # Shape: (batch_size, seq_len)\n",
    "\n",
    "pred_tags = []\n",
    "true_tags = []\n",
    "\n",
    "for pred, true_t in zip(predictions, val_tag_idx):\n",
    "    # print(pred.shape)\n",
    "    # print((len(true_tags)))\n",
    "    true_tags.extend([tag_vocab[t] for t in true_t])\n",
    "    pred_tags.extend([tag_vocab[t] for t in pred[: len(true_t)]])\n",
    "\n",
    "\n",
    "confmat = metrics.confusion_matrix(true_tags, pred_tags, labels=tag_vocab)\n",
    "\n",
    "acc = metrics.accuracy_score(true_tags, pred_tags)\n",
    "print(\"accuracy\", acc)\n",
    "\n",
    "pp.heatmap(\n",
    "    confmat,\n",
    "    # tag_vocab,\n",
    "    # log_scale=False,\n",
    "    # size=800,\n",
    ").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mango",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
