{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from importlib import reload\n",
    "\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn import metrics\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from datatools import tabular as dttab, plotting as dtplot\n",
    "\n",
    "\n",
    "import plotting\n",
    "import util\n",
    "\n",
    "reload(util)\n",
    "reload(plotting)\n",
    "\n",
    "dtplot.set_plotly_template()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = util.load_examples().sort(\"length\")\n",
    "\n",
    "lang_counts = dttab.value_counts(examples[\"lang\"], verbose=True, as_dict=True)\n",
    "print(lang_counts)\n",
    "# display(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-val split\n",
    "train_df, val_df = util.data_split(examples, 0.3)\n",
    "print(f\"split: {len(train_df)} training, {len(val_df)} val\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## most common tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_counts = dttab.value_counts(\n",
    "    examples[\"tokens\"].explode(), verbose=True, as_dict=True\n",
    ")\n",
    "tag_counts = dttab.value_counts(examples[\"tags\"].explode(), verbose=True, as_dict=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make a vocab!\n",
    "\n",
    "- add padding to both tokens and tags\n",
    "- also, convert tokens and tags to integers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab for tokens\n",
    "vocab = [\"<pad>\", \"<unk>\"] + list(token_counts.keys())[:10]\n",
    "token2idx = {t: i for i, t in enumerate(vocab)}\n",
    "\n",
    "# tags\n",
    "tag_vocab = [\"<pad>\"] + list(tag_counts.keys())\n",
    "tag2idx = {t: i for i, t in enumerate(tag_vocab)}\n",
    "\n",
    "print(\"vocab (tokens):\", vocab)\n",
    "print(\"vocab (tags)  :\", tag_vocab)\n",
    "\n",
    "# Convert tokens and labels to indices\n",
    "# these are lists of lists!\n",
    "train_token_idx = [[token2idx.get(t, 1) for t in seq] for seq in train_df[\"tokens\"]]\n",
    "train_tag_idx = [[tag2idx[t] for t in seq] for seq in train_df[\"tags\"]]\n",
    "# print(\"\\nlists of lists:\")\n",
    "# print(train_token_idx)\n",
    "# print(train_tag_idx)\n",
    "print(f\"\\ntraining examples of length: {[len(e) for e in train_token_idx]}\")\n",
    "\n",
    "# validation data\n",
    "val_token_idx = [[token2idx.get(t, 1) for t in seq] for seq in val_df[\"tokens\"]]\n",
    "val_tag_idx = [[tag2idx[t] for t in seq] for seq in val_df[\"tags\"]]\n",
    "print(f\"validation examples of length: {[len(e) for e in val_token_idx]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data for model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_token_tensors = util.seqs2padded_tensor(train_token_idx)\n",
    "train_tag_tensors = util.seqs2padded_tensor(train_tag_idx)\n",
    "val_token_tensors = util.seqs2padded_tensor(val_token_idx)\n",
    "val_tag_tensors = util.seqs2padded_tensor(val_tag_idx)\n",
    "\n",
    "print(f\"token tensor (train): {train_token_tensors.shape}\")\n",
    "print(f\"tag tensor   (train): {train_tag_tensors.shape}\")\n",
    "print(f\"token tensor (val): {val_token_tensors.shape}\")\n",
    "print(f\"tag tensor   (val): {val_tag_tensors.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, tokens, labels):\n",
    "        self.tokens = tokens\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.tokens[idx], self.labels[idx]\n",
    "\n",
    "\n",
    "# Create dataset and dataloader\n",
    "train_dataset = SequenceDataset(train_token_tensors, train_tag_tensors)\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "    def __init__(self, vocab_size, tagset_size, embedding_dim, hidden_dim):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, sentences):\n",
    "        embeds = self.embedding(sentences)\n",
    "        # Shape: (batch_size, seq_len, embedding_dim)\n",
    "        lstm_out, _ = self.lstm(embeds)\n",
    "        # Shape: (batch_size, seq_len, hidden_dim)\n",
    "        tag_scores = self.hidden2tag(lstm_out)\n",
    "        # Shape: (batch_size, seq_len, tagset_size)\n",
    "        return tag_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "embedding_dim = 32\n",
    "hidden_dim = 32\n",
    "vocab_size = len(vocab)\n",
    "tagset_size = len(tag_vocab)\n",
    "\n",
    "model = LSTMTagger(vocab_size, tagset_size, embedding_dim, hidden_dim)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "# Training loop\n",
    "epochs = 40\n",
    "losses = []\n",
    "for epoch in range(epochs):\n",
    "    for examples, labels in train_loader:\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        tag_scores = model(examples)\n",
    "\n",
    "        # Reshape for loss calculation\n",
    "        loss = loss_function(tag_scores.view(-1, tagset_size), labels.view(-1))\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    # print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}\")\n",
    "plotting.scatter(y=[losses])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    tag_scores = model(val_token_tensors)\n",
    "    predictions = torch.argmax(tag_scores, dim=-1)  # Shape: (batch_size, seq_len)\n",
    "\n",
    "pred_tags = []\n",
    "true_tags = []\n",
    "\n",
    "for pred, true_t in zip(predictions, val_tag_idx):\n",
    "    # print(pred.shape)\n",
    "    # print((len(true_tags)))\n",
    "    true_tags.extend([tag_vocab[t] for t in true_t])\n",
    "    pred_tags.extend([tag_vocab[t] for t in pred[: len(true_t)]])\n",
    "\n",
    "print(len(true_tags), len(pred_tags))\n",
    "\n",
    "\n",
    "def eval(true_tags, pred_tags):\n",
    "    acc = metrics.accuracy_score(true_tags, pred_tags)\n",
    "    print(\"accuracy\", acc)\n",
    "\n",
    "    confmat = metrics.confusion_matrix(true_tags, pred_tags, labels=tag_vocab)\n",
    "\n",
    "    dtplot.heatmap(\n",
    "        confmat,\n",
    "        tag_vocab,\n",
    "        log_scale=True,\n",
    "        pseudo_count=10,\n",
    "        size=500,\n",
    "    ).show()\n",
    "\n",
    "\n",
    "eval(true_tags, pred_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import text_process\n",
    "\n",
    "pred_tags_det = (\n",
    "    val_df[\"tokens\"]\n",
    "    .map_elements(lambda tks: text_process.process(\"\".join(tks))[1], pl.List(pl.String))\n",
    "    .explode()\n",
    "    .to_list()\n",
    ")\n",
    "\n",
    "# put all unknown into some class\n",
    "# pred_tags_det = [t if t != \"uk\" else \"va\" for t in pred_tags_det]\n",
    "\n",
    "print(len(true_tags), len(pred_tags_det))\n",
    "print(true_tags)\n",
    "print(pred_tags_det)\n",
    "\n",
    "\n",
    "eval(true_tags, pred_tags_det)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## simple fill\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_filled = []\n",
    "for p, pd in zip(pred_tags, pred_tags_det, strict=True):\n",
    "    pred_filled.append(pd if pd != \"uk\" else p)\n",
    "\n",
    "print(pred_filled)\n",
    "eval(true_tags, pred_filled)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "_my_python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
